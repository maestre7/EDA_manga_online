{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path_root = Path(\"../../data/data_chapters/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['demography', 'type', 'uuid'], dtype='object')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recuperamos el dataset base ya filtrado\n",
    "dataset_df = pd.read_csv(\"../../data/dataset_4.csv\", usecols = [\"uuid\",\"type\",\"demography\"]) # index_col=0)\n",
    "dataset_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nos quedamos con las columnas idetificadoras para el cruce\n",
    "data_search_df = dataset_df[[\"uuid\",\"type\",\"demography\"]]\n",
    "#data_search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\usuario\\Documents\\GitHub\\EDA_manga_online\\src\\data\\data_chapters\n"
     ]
    }
   ],
   "source": [
    "# Preparamos el path root de los capitulos\n",
    "os.chdir(path_root)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperamos todos los sub folders de capitulos\n",
    "folders_path = []\n",
    "for i in os.listdir():\n",
    "    if '.' not in i:\n",
    "        folders_path.append(os.path.join(path_root, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19084"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recuperamos todos los path de los fichero de capitulos\n",
    "files = []\n",
    "for folder_path in folders_path:\n",
    "    tree_file = os.walk(folder_path)\n",
    "    for data_file in tree_file:\n",
    "        tree_folder = data_file[0]\n",
    "        files.extend([os.path.join(tree_folder, file) for file in data_file[2]])\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe base\n",
    "file_uuid = files[0].split(\"\\\\\")[-1].split(\".\")[0]\n",
    "chapter_df = pd.read_csv(files[0], usecols = [\"date_upload\"])\n",
    "key = dataset_df[dataset_df[\"uuid\"] == file_uuid]\n",
    "chapter_df[\"uuid\"] = file_uuid\n",
    "dataset_chapters_df = pd.merge(chapter_df, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "# Ojo q tarda 3 min o mas\n",
    "# Cargamos el resto de los datos\n",
    "no_upload_date = 0\n",
    "for file in files[1:]: \n",
    "    try:\n",
    "        chapter_df = pd.read_csv(file, usecols = [\"date_upload\"])  \n",
    "    except ValueError as err: # ignoramos los que no tienen fecha de subida\n",
    "        no_upload_date += 1\n",
    "    else:\n",
    "        file_uuid = file.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        key = dataset_df[dataset_df[\"uuid\"] == file_uuid]\n",
    "        chapter_df[\"uuid\"] = file_uuid\n",
    "        result = pd.merge(chapter_df, key)\n",
    "        dataset_chapters_df = pd.concat([dataset_chapters_df, result])\n",
    "\n",
    "print(no_upload_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiamos un espacio al inicio del campo\n",
    "dataset_chapters_df[\"date_upload\"] = [row.replace(\" \", \"\") for row in dataset_chapters_df[\"date_upload\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos los registros con mas de una subida por capitulo\n",
    "dataset_multy_chapters_df = dataset_chapters_df[dataset_chapters_df[\"date_upload\"].str.len() > 10].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575336\n",
      "575336\n"
     ]
    }
   ],
   "source": [
    "# Dejamos una sola fecha para los capitulos que tienen mas de una subida. ###\n",
    "print(len(dataset_chapters_df))\n",
    "dataset_chapters_df[\"date_upload\"] = dataset_chapters_df[\"date_upload\"].apply(lambda data : data.split(\",\")[0] if len(data) > 10 else data)\n",
    "print(len(dataset_chapters_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575192\n"
     ]
    }
   ],
   "source": [
    "# Eliminados las fechas errornes = '-0001-11-30'\n",
    "dataset_chapters_df = dataset_chapters_df[dataset_chapters_df.date_upload.str.len() == 10]\n",
    "print(len(dataset_chapters_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72936\n",
      "86124\n"
     ]
    }
   ],
   "source": [
    "# Desglosamos los capitulos con subidas multiples a un resgitro por subida\n",
    "list_chapters = []\n",
    "print(len(dataset_multy_chapters_df))\n",
    "for index, row in dataset_multy_chapters_df.iterrows():\n",
    "    list_date_upload = row[\"date_upload\"].split(\",\")[1:]\n",
    "    for date in list_date_upload:\n",
    "        list_chapters.append({\"date_upload\": date, \n",
    "                              \"uuid\": row[\"uuid\"], \n",
    "                              \"demography\": row[\"demography\"],\n",
    "                              \"type\": row[\"type\"]})\n",
    "        \n",
    "print(len(list_chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86124\n"
     ]
    }
   ],
   "source": [
    "dataset_multy_chapters = pd.DataFrame(list_chapters)\n",
    "print(len(dataset_multy_chapters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chapters_df.to_csv(\"../../data/dataset_chapters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_chapters_df.to_csv(\"../../data/dataset_multy_chapters.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
